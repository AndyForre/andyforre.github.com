{
  "name": "Andyforre.GitHub.com",
  "tagline": "Assignment - machine learning",
  "body": "###1. Data import and selection of predictors\r\nThe first part of this report describes selection of predictor variables for use during model construction. Reducing the number of predictor variables is an important step in order to remove random noise in the dataset. Several different strategies for reducing number of features exists. Below follows an explanation of the strategy used in this assignment.\r\nThe training dataset for the assignment was obtained from the following url: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv. Data was downloaded and imported into R studio using the following code:\r\n     \r\n     url1<- \"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv\"\r\n     download.file(url1,destfile=\"./trainset.csv\")\r\n     training = read.csv(\"trainset.csv\", header=T, na.strings = c(\"NA\",\"\", \" \",\"#DIV/0!\") )\r\n     url2<- \"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv \"\r\n     download.file(url2,destfile=\"./validationset.csv\")\r\n     testing = read.csv(\"validationset.csv\",header=T, na.strings = c(\"NA\",\"\", \" \",\"#DIV/0!\") )\r\n\r\nThe trainingdata is a matrix comprising 19622 observations of 160 variables. The first step to select important predictor variables was to remove predictors that did not contain any values, i.e. the columns containing the NA values. The following code was used to remove predictors containing more than one NA value: \r\n\r\n    RemovedNATrain <-which(colSums(is.na(training))>1)\r\n    training1 <-training[,-c(RemovedNATrain)]\r\n    dim(training1)\r\n    [1] 19622    60\r\n \r\nA total of 100 predictors were removed. The new data matrix comprised 19622 observations of 60 variables. Next step in predictor selection involved manual inspection of the predictors. The first seven predictors contained annotation information, hence likely not related to the predicted outcome. These were removed using the following code:\r\n\r\n    training2 <- subset( training1, select = -c( X : num_window ))\r\n\r\nThe new data matrix comprised 19622 observations of 53 variables. Next step involved identification of near zero variance predictors, since these would most certainly not be useful covariates:\r\n\r\n    library(caret)\r\n    TestNearZero <- nearZeroVar(training2[sapply(training2,is.numeric)],saveMetrics = TRUE)\r\n    nzv(training2, freqCut = 95/5, uniqueCut = 10, saveMetrics = FALSE, names = FALSE)\r\n    integer(0)\r\n\r\nNo samples were removed, all samples had enough variability. Next step was to remove highly correlated numerical features. This were used by first generating a correlation matrix and then use the findCorrelation function within the caret package to remove highly correlated predictors:\r\n\r\n    CorMat <-abs(cor(training2[sapply(training2, is.numeric)]))\r\n    r = findCorrelation(CorMat, cutoff = .90, verbose = TRUE)\r\n    training3 = training2[,-r]\r\n\r\nThe new data matrix comprised 19622 observations of 46 variables. This was the last step in the predictor selection process. The 46 predictor variables were used as input into the model. \r\n\r\n###2. Construction and optimization of prediction model\r\nThe second part of this report describes construction of the predictive model. In order to achieve accurate predictions of unknown samples, it is important to tune and optimize parameters on a selected model. It is important that all such optimization steps are performed only on samples in training data in order not to risk overfitting when generalizing to new samples. In order to tune our model and to estimate model performance, we split the trainingdata into two fractions: a training fraction (75% of samples) containing the samples used to train the model and to estimate the tuning parameters, and a test set (25% of samples) to evaluate the ability of our model to generalize to unseen samples.\r\n\r\n    set.seed(2) \r\n    train <-createDataPartition(y=training3$classe,p=0.75,list=FALSE)\r\n    trainset<-training3[train,]\r\n    testset<-training3[-train,]\r\n    dim(trainset)\r\n    dim(testset)\r\n    [1] 14718    46\r\n    [1] 4904   46\r\n\r\nDue to personal preferences, a Random Forest (RF) model was first fitted to the samples in the trainingset. The only available tuning parameter was mtry, the number of variables randomly sampled as candidates at each split. \r\n\r\n    library(randomForest)\r\n    set.seed(2)\r\n    ctrl = trainControl(method = \"cv\",number=3, allowParallel=T)\r\n    rf.caret.model = train(classe~., data=trainset, method=\"rf\",trControl=ctrl, tuneGrid=expand.grid(.mtry=1:7),verbose=FALSE)\r\n    rf.caret.model\r\n\r\n     Random Forest \r\n\r\n      14718 samples\r\n        45 predictor\r\n         5 classes: 'A', 'B', 'C', 'D', 'E' \r\n\r\n    No pre-processing\r\n    Resampling: Cross-Validated (3 fold) \r\n    Summary of sample sizes: 9812, 9812, 9812 \r\n    Resampling results across tuning parameters:\r\n\r\n       mtry  Accuracy   Kappa      Accuracy SD   Kappa SD   \r\n      1     0.9816551  0.9767857  0.0006114961  0.000771878\r\n      2     0.9878380  0.9846135  0.0019797258  0.002505088\r\n      3     0.9896046  0.9868497  0.0013366161  0.001691749\r\n      4     0.9900122  0.9873657  0.0015388982  0.001946648\r\n      5     0.9913032  0.9889991  0.0013874560  0.001754146\r\n      6     0.9914391  0.9891708  0.0008153282  0.001029852\r\n      7     0.9908955  0.9884833  0.0013571806  0.001716210\r\n\r\n    Accuracy was used to select the optimal model using  the largest value.\r\n    The final value used for the model was mtry = 6.\r\n\r\nDue to limitations in time, a 3-fold cross-validation procedure was first used on samples in trainingset to determine the mtry parameter. The number of mtry to evaluate was set to all integers between one and the square root of the number of predictors (√46 = 7). The best performing value of mtry was determined based on highest accuracy. This step could be repeated if the performance on test samples was not sufficient.\r\n\r\n###3.) Results – Prediction of samples in test set\r\n\r\nSamples in the testset set were predicted using the tuned model.\r\n\r\n        pred1 = predict(rf.caret.model, newdata=testset)\r\n        confusionMatrix(pred1, testset$classe) \r\n        Accuracy : 0.9953\r\n                            Class: A Class: B Class: C Class: D Class: E\r\n        Sensitivity            1.0000   0.9905   0.9930   0.9913   0.9989\r\n        Specificity            0.9977   0.9985   0.9985   0.9998   0.9995\r\n        Pos Pred Value         0.9943   0.9937   0.9930   0.9987   0.9978\r\n        Neg Pred Value         1.0000   0.9977   0.9985   0.9983   0.9998\r\n        Prevalence             0.2845   0.1935   0.1743   0.1639   0.1837\r\n        Detection Rate         0.2845   0.1917   0.1731   0.1625   0.1835\r\n        Detection Prevalence   0.2861   0.1929   0.1743   0.1627   0.1839\r\n        Balanced Accuracy      0.9989   0.9945   0.9958   0.9955   0.9992\r\n   \r\nFor prediction of samples in the left out test set, the model reached an impressive accuracy of 99.5%. This means that we do not need to optimize model or try other models. Since compounds in left out test set was not used during model construction or optimization, the accuracy is a valid estimate of the out of sample accuracy. \r\n\r\n###4.) Results – Prediction of samples in independent validationset\r\n\r\nSamples in the independent test set were predicted with the same model:\r\n     \r\n        Finaltest<-testing[,names(training3[,-46])]\r\n        dim(Finaltest)\r\n        pred2 = predict(rf.caret.model, newdata=Finaltest)\r\n        Answer: B A B A A E D B A A B C B A E E A B B B\r\n        Levels: A B C D E\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}